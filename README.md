# Machine-Learning 
COMS 4771 taught by Professor Nakul Verma

Topics covored: Maximum Likelihood Estimation, Classification via Probabilistic Modeling, Bayes Classifier, Naive Bayes, Evaluating Classifiers, Generative vs. Discriminative classifiers, Nearest Neighbor classifier, Coping with drawbacks of k-NN, Decision Trees, Model Complexity and Overfitting, Decision boundaries for classification, Linear decision boundaries (Linear classification), The Perceptron algorithm, Coping with non-linear boundaries, Kernel feature transform, Kernel trick, Support Vector Machines, Large margin formulation, Constrained Optimization, Lagrange Duality, Convexity, Duality Theorems, Dual SVMs, Regression, Parametric vs. non-parametric regression, Ordinary least squares regression, Logistic regression, Lasso and ridge regression, Optimal regressor, Kernel regression, consistency of kernel regression, Statistical theory of learning, PAC-learnability, Occam's razor theorem, VC dimension, VC theorem, Concentration of measure, Unsupervised Learning, Clustering, k-means, Hierarchical clustering, Gaussian mixture modeling, Expectation Maximization Algorithm, Dimensionality Reduction, Principal Components Analysis (PCA), non-linear dimension reduction (manifold learning), Graphical Models, Bayesian Networks, Markov Random Fields, Inference and learning on graphical models, Markov Chains, and Hidden Markov Models (HMMs).

The written homework solutions are for the theoretical part of the homework plus some analysis of the performance of the coding section of the homework.
